# run_pipeline.py
import subprocess
import sys
import os
import time
from pathlib import Path
import argparse

def print_header(title):
    """Affiche un en-tÃªte stylisÃ©"""
    print("\n" + "="*60)
    print(f"ğŸš€ {title}")
    print("="*60)

def run_command(cmd, description=None):
    """ExÃ©cute une commande shell"""
    if description:
        print(f"\nâ–¶ï¸  {description}")
        print(f"   Commande: {' '.join(cmd)}")
    
    result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
    
    if result.returncode == 0:
        print(f"   âœ… SuccÃ¨s")
        if result.stdout.strip():
            print(f"   Sortie: {result.stdout.strip()[:200]}...")
        return True
    else:
        print(f"   âŒ Erreur (code: {result.returncode})")
        if result.stderr.strip():
            print(f"   Erreur: {result.stderr.strip()[:200]}...")
        return False

def check_dependencies():
    """VÃ©rifie les dÃ©pendances"""
    print_header("VÃ‰RIFICATION DES DÃ‰PENDANCES")
    
    dependencies = [
        ("Python", ["python", "--version"]),
        ("DVC", ["dvc", "--version"]),
        ("MLflow", ["mlflow", "--version"]),
        ("pip", ["pip", "--version"])
    ]
    
    all_ok = True
    for name, cmd in dependencies:
        if run_command(cmd, f"VÃ©rification {name}"):
            print(f"   âœ“ {name} installÃ©")
        else:
            print(f"   âœ— {name} non installÃ©")
            all_ok = False
    
    return all_ok

def run_dvc_pipeline():
    """ExÃ©cute le pipeline DVC complet"""
    print_header("EXÃ‰CUTION DU PIPELINE DVC")
    
    print("\nğŸ“Š Ã‰tat actuel du pipeline:")
    run_command(["dvc", "dag"], "Visualisation du pipeline")
    
    print("\nğŸ”„ ExÃ©cution du pipeline...")
    if run_command(["dvc", "repro"], "Pipeline DVC"):
        print("\nâœ… Pipeline exÃ©cutÃ© avec succÃ¨s!")
        
        # Afficher les rÃ©sultats
        print("\nğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S:")
        if os.path.exists("models/"):
            models = os.listdir("models")
            print(f"   models/: {len(models)} fichiers")
        
        if os.path.exists("reports/"):
            reports = os.listdir("reports")
            print(f"   reports/: {len(reports)} fichiers")
        
        return True
    else:
        print("\nâŒ Erreur lors de l'exÃ©cution du pipeline")
        return False

def train_single_model(model_type="logistic"):
    """EntraÃ®ne un modÃ¨le spÃ©cifique"""
    print_header(f"ENTRAÃNEMENT MODÃˆLE: {model_type.upper()}")
    
    model_file = f"models/model_{model_type}.pkl"
    
    cmd = [
        "python", "src/models/train.py",
        "--data", "data/processed/sms_clean.csv",
        "--out", model_file,
        "--model_type", model_type
    ]
    
    if run_command(cmd, f"EntraÃ®nement {model_type}"):
        print(f"\nâœ… ModÃ¨le sauvegardÃ©: {model_file}")
        return True
    return False

def train_all_models():
    """EntraÃ®ne tous les modÃ¨les disponibles"""
    print_header("ENTRAÃNEMENT MULTI-MODÃˆLES")
    
    models = ["logistic", "svm", "nb", "rf"]
    results = {}
    
    for model_type in models:
        print(f"\nğŸ§  {model_type.upper()}...")
        success = train_single_model(model_type)
        results[model_type] = success
        time.sleep(1)  # Pause entre les entraÃ®nements
    
    # RÃ©sumÃ©
    print("\nğŸ“Š RÃ‰SUMÃ‰ DE L'ENTRAÃNEMENT:")
    successful = [m for m, s in results.items() if s]
    failed = [m for m, s in results.items() if not s]
    
    if successful:
        print(f"   âœ… RÃ©ussi: {', '.join(successful)}")
    if failed:
        print(f"   âŒ Ã‰chouÃ©: {', '.join(failed)}")
    
    return len(failed) == 0

def compare_models():
    """Compare tous les modÃ¨les entraÃ®nÃ©s"""
    print_header("COMPARAISON DES MODÃˆLES")
    
    if not os.path.exists("models/"):
        print("âŒ Aucun modÃ¨le trouvÃ©. EntraÃ®nez d'abord des modÃ¨les.")
        return False
    
    cmd = [
        "python", "src/models/evaluate_all.py",
        "--models_dir", "models/",
        "--data", "data/processed/sms_clean.csv",
        "--out", "reports/model_comparison.json"
    ]
    
    if run_command(cmd, "Comparaison des modÃ¨les"):
        print("\nğŸ“Š RÃ‰SULTATS DE COMPARAISON:")
        
        # Lire et afficher le rapport
        report_path = "reports/model_comparison.json"
        if os.path.exists(report_path):
            import json
            with open(report_path, 'r') as f:
                report = json.load(f)
            
            if "best_model" in report:
                best = report["best_model"]
                print(f"   ğŸ† Meilleur modÃ¨le: {best['name']}")
                print(f"   ğŸ“ˆ F1-Score: {best['metrics']['f1_score']:.4f}")
                print(f"   ğŸ¯ Accuracy: {best['metrics']['accuracy']:.4f}")
        
        return True
    return False

def start_mlflow_ui():
    """DÃ©marre l'interface MLflow"""
    print_header("INTERFACE MLFLOW")
    
    print("ğŸŒ DÃ©marrage de MLflow UI sur http://localhost:5000")
    print("   Appuyez sur Ctrl+C pour arrÃªter")
    
    try:
        # DÃ©marrer MLflow en arriÃ¨re-plan
        import threading
        
        def run_mlflow():
            subprocess.run(["mlflow", "ui", "--port", "5000", "--host", "0.0.0.0"])
        
        thread = threading.Thread(target=run_mlflow, daemon=True)
        thread.start()
        
        print("âœ… MLflow dÃ©marrÃ©")
        print("   AccÃ©dez Ã : http://localhost:5000")
        print("\nâ³ Attente de 5 secondes pour le dÃ©marrage...")
        time.sleep(5)
        
        # Essayer d'ouvrir le navigateur
        try:
            import webbrowser
            webbrowser.open("http://localhost:5000")
        except:
            pass
        
        # Garder le script en vie
        input("\nğŸ¯ Appuyez sur EntrÃ©e pour continuer...")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ MLflow arrÃªtÃ©")
    except Exception as e:
        print(f"âŒ Erreur: {e}")

def clean_outputs():
    """Nettoie les rÃ©pertoires de sortie"""
    print_header("NETTOYAGE DES SORTIES")
    
    dirs_to_clean = ["models", "reports", "artifacts", "logs"]
    
    for dir_name in dirs_to_clean:
        if os.path.exists(dir_name):
            import shutil
            try:
                shutil.rmtree(dir_name)
                print(f"   ğŸ—‘ï¸  SupprimÃ©: {dir_name}/")
            except Exception as e:
                print(f"   âŒ Erreur suppression {dir_name}: {e}")
        else:
            print(f"   âœ“ DÃ©jÃ  propre: {dir_name}/")
    
    print("\nâœ… Nettoyage terminÃ©")

def main_menu():
    """Menu principal interactif"""
    while True:
        print_header("MLOps PIPELINE - SPAM DETECTION")
        
        print("1. ğŸ”„ ExÃ©cuter le pipeline DVC complet")
        print("2. ğŸ¤– EntraÃ®ner tous les modÃ¨les")
        print("3. ğŸ§  EntraÃ®ner un modÃ¨le spÃ©cifique")
        print("4. ğŸ“Š Comparer les modÃ¨les")
        print("5. ğŸ“ˆ DÃ©marrer MLflow UI")
        print("6. ğŸ§¹ Nettoyer les sorties")
        print("7. âœ… VÃ©rifier les dÃ©pendances")
        print("8. ğŸšª Quitter")
        
        choice = input("\nğŸ‘‰ Choix (1-8): ").strip()
        
        if choice == "1":
            if check_dependencies():
                run_dvc_pipeline()
        elif choice == "2":
            train_all_models()
        elif choice == "3":
            model_type = input("Type de modÃ¨le (logistic/svm/nb/rf): ").strip().lower()
            if model_type in ["logistic", "svm", "nb", "rf"]:
                train_single_model(model_type)
            else:
                print("âŒ Type de modÃ¨le invalide")
        elif choice == "4":
            compare_models()
        elif choice == "5":
            start_mlflow_ui()
        elif choice == "6":
            clean_outputs()
        elif choice == "7":
            check_dependencies()
        elif choice == "8":
            print("\nğŸ‘‹ Au revoir!")
            break
        else:
            print("âŒ Choix invalide")
        
        input("\nâ Appuyez sur EntrÃ©e pour continuer...")

def main():
    """Point d'entrÃ©e principal"""
    parser = argparse.ArgumentParser(description="Pipeline MLOps pour dÃ©tection de spam")
    parser.add_argument("--mode", choices=["auto", "menu", "dvc", "train", "compare", "clean"],
                       default="menu", help="Mode d'exÃ©cution")
    parser.add_argument("--model", choices=["logistic", "svm", "nb", "rf"],
                       help="Type de modÃ¨le pour l'entraÃ®nement")
    
    args = parser.parse_args()
    
    if args.mode == "auto":
        # Mode automatique: vÃ©rif â†’ pipeline â†’ comparaison
        print_header("MODE AUTOMATIQUE")
        if check_dependencies():
            run_dvc_pipeline()
            compare_models()
    
    elif args.mode == "dvc":
        run_dvc_pipeline()
    
    elif args.mode == "train":
        if args.model:
            train_single_model(args.model)
        else:
            train_all_models()
    
    elif args.mode == "compare":
        compare_models()
    
    elif args.mode == "clean":
        clean_outputs()
    
    else:  # menu par dÃ©faut
        main_menu()

if __name__ == "__main__":
    main() 
